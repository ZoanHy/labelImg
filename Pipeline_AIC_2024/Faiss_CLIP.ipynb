{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create all image path",
   "id": "ca5bb1e09fa7ee0a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-01T10:28:59.965971Z",
     "start_time": "2024-09-01T10:28:59.456702Z"
    }
   },
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "jpg_files_path = glob.glob('images/Keyframes/**/keyframes/**/*.jpg', recursive=True)\n",
    "\n",
    "jpg_dict = {i:jpg_path for i, jpg_path in enumerate(jpg_files_path)}\n",
    "\n",
    "jpg_sorted_dict = dict(sorted(jpg_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "reset_jpg_sorted_dict = {i: v for i, (k, v) in enumerate(jpg_sorted_dict.items())}\n",
    "\n",
    "for i, value in reset_jpg_sorted_dict.items():\n",
    "    print(i, value)\n",
    "\n",
    "    if i == 5:\n",
    "        break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images/Keyframes/Keyframes_L01/keyframes/L01_V001/001.jpg\n",
      "1 images/Keyframes/Keyframes_L01/keyframes/L01_V001/002.jpg\n",
      "2 images/Keyframes/Keyframes_L01/keyframes/L01_V001/003.jpg\n",
      "3 images/Keyframes/Keyframes_L01/keyframes/L01_V001/004.jpg\n",
      "4 images/Keyframes/Keyframes_L01/keyframes/L01_V001/005.jpg\n",
      "5 images/Keyframes/Keyframes_L01/keyframes/L01_V001/006.jpg\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:29:31.461292Z",
     "start_time": "2024-09-01T10:29:31.380939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_name = \"image_all_path.json\"\n",
    "\n",
    "# Open the file in write mode and write the JSON data\n",
    "with open(file_name, 'w') as json_file:\n",
    "    json.dump(reset_jpg_sorted_dict, json_file, indent=4)"
   ],
   "id": "2ca0fbb3701c4b90",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create faiss bin for keyframes",
   "id": "4f4c594f013b4157"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:30:01.764211Z",
     "start_time": "2024-09-01T10:30:01.761031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ],
   "id": "f39e4c8d7bb17211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:30:08.436950Z",
     "start_time": "2024-09-01T10:30:04.708318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "id": "17a692e45e278116",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:30:10.078544Z",
     "start_time": "2024-09-01T10:30:10.073538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if the model is on the GPU\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print(\"The model is running on GPU.\")\n",
    "else:\n",
    "    print(\"The model is running on CPU.\")\n"
   ],
   "id": "8e107075052e6f56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is running on CPU.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:30:11.931398Z",
     "start_time": "2024-09-01T10:30:11.927452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def embed_images(image_paths):\n",
    "    # Load and preprocess images\n",
    "    images = [Image.open(path) for path in image_paths]\n",
    "    \n",
    "    # Preprocess the images using the CLIP processor and move to GPU\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move inputs to the same device as the model (GPU if available)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate embeddings using the GPU\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    # Normalize the embeddings\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Convert the embeddings to CPU before converting to NumPy, to avoid GPU memory issues\n",
    "    return image_features.cpu().numpy()\n"
   ],
   "id": "966a12cee1bc5e3d",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T10:30:15.533625Z",
     "start_time": "2024-09-01T10:30:15.480120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "json_file_path = 'image_all_path.json'\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    image_paths = json.load(file)\n",
    "\n",
    "# Print the dictionary to verify\n",
    "# print(image_paths)\n"
   ],
   "id": "2cb762ef22017e90",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-01T10:30:18.015978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize FAISS index (assuming we have 512-dim CLIP features)\n",
    "dimension = 512  # CLIP model output dimension\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Batch processing (if you have a large number of images, process them in batches)\n",
    "batch_size = 16  # Adjust the batch size depending on your memory\n",
    "image_keys = list(image_paths.keys())  # Get all keys from the dictionary\n",
    "\n",
    "for i in tqdm(range(0, len(image_keys), batch_size), desc=\"Embedding Images\"):\n",
    "    # Get the current batch of image paths\n",
    "    batch_keys = image_keys[i:i + batch_size]\n",
    "    batch_paths = [image_paths[key] for key in batch_keys]\n",
    "    \n",
    "    # Embed the images using the GPU\n",
    "    embeddings = embed_images(batch_paths)\n",
    "    \n",
    "    # Add the batch of embeddings to the FAISS index\n",
    "    index.add(embeddings)\n",
    "\n",
    "print(f\"Total embeddings indexed: {index.ntotal}\")\n"
   ],
   "id": "4008670b849ec35a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Images:   0%|          | 7/6662 [00:05<1:29:36,  1.24it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the FAISS index to a file\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n"
   ],
   "id": "87e4c72235536977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the FAISS index from a file\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "index"
   ],
   "id": "e746b5675acff610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a13da8f34fe4d7cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
